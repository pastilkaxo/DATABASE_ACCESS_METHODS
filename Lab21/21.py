import mglearn
import sklearn
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC



# yÀÜ ùë§[0]‚àóùë•[0]	ùë§[1]‚àóùë•[1] ‚Ä¶ ùë§[ùëù]‚àóùë•[ùëù] ùëè
#  –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
# –ø–æ—Ä–æ–≥, —Ä–∞–≤–Ω—ã–π –Ω—É–ª—é. –ï—Å–ª–∏ —Ñ—É–Ω–∫—Ü–∏—è –º–µ–Ω—å—à–µ –Ω—É–ª—è, –º—ã –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º –∫–ª–∞—Å—Å ‚Äì1, –µ—Å–ª–∏ –æ–Ω–∞ –±–æ–ª—å—à–µ
# –Ω—É–ª—è, –º—ã –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º –∫–ª–∞—Å—Å +1.
#

# –î–ª—è –ª–∏–Ω–µ–π–Ω—ã—Ö
# –º–æ–¥–µ–ª–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≥—Ä–∞–Ω–∏—Ü–∞ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π (decision boundary) —è–≤–ª—è–µ—Ç—Å—è –ª–∏–Ω–µ–π–Ω–æ–π
# —Ñ—É–Ω–∫—Ü–∏–µ–π –∞—Ä–≥—É–º–µ–Ω—Ç–∞. –î—Ä—É–≥–∏–º–∏ —Å–ª–æ–≤–∞–º–∏, (–±–∏–Ω–∞—Ä–Ω—ã–π) –ª–∏–Ω–µ–π–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä ‚Äì —ç—Ç–æ
# –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–∑–¥–µ–ª—è–µ—Ç –¥–≤–∞ –∫–ª–∞—Å—Å–∞ —Å –ø–æ–º–æ—â—å—é –ª–∏–Ω–∏–∏, –ø–ª–æ—Å–∫–æ—Å—Ç–∏ –∏–ª–∏ –≥–∏–ø–µ—Ä–ø–ª–æ—Å–∫–æ—Å—Ç–∏. –í
# —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã –ø—Ä–∏–≤–µ–¥–µ–º –∫–æ–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã.

# –î–≤—É–º—è –Ω–∞–∏–±–æ–ª–µ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏ –ª–∏–Ω–µ–π–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —è–≤–ª—è—é—Ç—Å—è
# –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è (logistic regression), —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –≤ –∫–ª–∞—Å—Å–µ
# linear_model.LogisticRegression, –∏ –ª–∏–Ω–µ–π–Ω—ã–π –º–µ—Ç–æ–¥ –æ–ø–æ—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ (linear support vector
# machines) –∏–ª–∏ –ª–∏–Ω–µ–π–Ω—ã–π SVM, —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –≤ –∫–ª–∞—Å—Å–µ svm.LinearSVC (SVC —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤—ã–≤–∞–µ—Ç—Å—è –∫–∞–∫
# support vector classifier ‚Äì –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –æ–ø–æ—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤).
#

X, y = mglearn.datasets.make_forge()
fig, axes = plt.subplots(1, 2, figsize=(10, 3))
for model, ax in zip([LinearSVC(), LogisticRegression()], axes):
 clf = model.fit(X, y)
 mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5,
 ax=ax, alpha=.7)
 mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)
 ax.set_title("{}".format(clf.__class__.__name__))
 ax.set_xlabel("–ü—Ä–∏–∑–Ω–∞–∫ 0")
 ax.set_ylabel("–ü—Ä–∏–∑–Ω–∞–∫ 1")
axes[0].legend()
plt.show()




mglearn.plots.plot_linear_svc_regularization()
plt.show()

from sklearn.datasets import load_breast_cancer
cancer = load_breast_cancer()
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
 cancer.data, cancer.target, stratify=cancer.target, random_state=42)
logreg = LogisticRegression().fit(X_train, y_train)
print("============== C = 1 =======")
print("–ü—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–∞—é—â–µ–º –Ω–∞–±–æ—Ä–µ: {:.3f}".format(logreg.score(X_train, y_train)))
print("–ü—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ: {:.3f}".format(logreg.score(X_test, y_test)))
print("============== C = 100 =======")
logreg100 = LogisticRegression(C=100).fit(X_train, y_train)
print("–ü—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–∞—é—â–µ–º –Ω–∞–±–æ—Ä–µ: {:.3f}".format(logreg100.score(X_train, y_train)))
print("–ü—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ: {:.3f}".format(logreg100.score(X_test, y_test)))
print("============== C = 0.01 =======")
logreg001 = LogisticRegression(C=0.01).fit(X_train, y_train)
print("–ü—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–∞—é—â–µ–º –Ω–∞–±–æ—Ä–µ: {:.3f}".format(logreg001.score(X_train, y_train)))
print("–ü—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ: {:.3f}".format(logreg001.score(X_test, y_test)))
plt.plot(logreg.coef_.T, 'o', label="C=1")
plt.plot(logreg100.coef_.T, '^', label="C=100")
plt.plot(logreg001.coef_.T, 'v', label="C=0.001")
plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)
plt.hlines(0, 0, cancer.data.shape[1])
plt.ylim(-5, 5)
plt.xlabel("–ò–Ω–¥–µ–∫—Å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞")
plt.ylabel("–û—Ü–µ–Ω–∫–∞ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞")
plt.legend()
plt.show()


